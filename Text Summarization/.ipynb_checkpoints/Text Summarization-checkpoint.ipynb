{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was published at https://github.com/duyvuleo/VNTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will implement some algorithms to apply in text summarization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Text Summarization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text summarization is the problem of creating a short, accurate, and fluent summary of a longer text document.\n",
    "\n",
    "Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online to both better help discover relevant information and to consume relevant information faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we do in this tutorial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will solve Text Summarization for Vietnamese newspapers, using some algorithms belows:\n",
    "1. Extractive Text Summarization\n",
    "    - Doc2Vec\n",
    "    - Text Rank\n",
    "2. Abstractive Text Summarization\n",
    "    - Google textsum\n",
    "\n",
    "\n",
    "We just implement \"**Single document summarization**\" problem in this tutorial, another problem called \"**Multi-document summarization**\" will be dicussed in another time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic idea\n",
    "The idea of using Doc2Vec algorithm for text summarization problem is described as follows:\n",
    "1. In all documents, we will extract sentences separately.\n",
    "2. Each sentence will be represented by a vector, via doc2vec model\n",
    "3. Use KMean algorithm to find out most featured sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "dir_path = os.path.join(dir_path, 'Data')\n",
    "\n",
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_data(folder):\n",
    "    sentences = []\n",
    "    for path in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, path)\n",
    "        with open(file_path, 'r', encoding=\"utf-16\") as f:\n",
    "\n",
    "            lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                sens = line.split('.')\n",
    "                for sen in sens:\n",
    "                    if len(sen) > 10:\n",
    "                        sen = gensim.utils.simple_preprocess(sen)\n",
    "                        sen = ' '.join(sen)\n",
    "                        sen = ViTokenizer.tokenize(sen)\n",
    "                        sentences.append(sen)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = 'Tottenham mời Roberto Mancini làm HLV. Athole Still - đại diện của chiến lược gia người Italy - mới tiết lộ rằng Mancini đang nằm trong tầm ngắm của CLB phía bắc thành London. Tuy nhiên, Still cũng khẳng định Tottenham phải đợi đến hết mùa hè năm 2004 nếu còn muốn có chữ ký của thân chủ mình. Những tin đồn về việc Chủ tịch Daniel Levy tìm người lèo lái con thuyền Tottenham đang gặp hiểm nguy càng trở nên có cơ sở, khi HLV tạm quyền David Pleat không thể cải thiện chuỗi thành tích nghèo nàn của đội bóng bóng tại giải vô địch quốc gia. Hiện đứng vị trí thứ 17 với 18 điểm sau 19 vòng đấu, Tottenham đang lâm vào tình trạng vô cùng khó khăn. Tottenham đã liên hệ với những HLV hàng đầu châu Âu như Martin ONeill, Alan Curbishley và Guus Hiddink. Tuy nhiên, Roberto Mancini là cái tên nhận được sự tin tưởng nhất.Trong khi đó, ông Athole Still - đại diện của HLV Roberto Mancini -cho biết: \"Qua các phương tiện thông tin đại chúng, tôi biết rằng Tottenham đang quan tâm đến Roberto. Nhưng chúng tôi chưa nhận được một lời mời chính thức nào. Phải nói thêm rằng Mancini rất yêu quý nước Anh và cậu ấy nói tiếng Anh rất khá. Mancini cũng tích luỹ được khá nhiều kinh nghiệm và cũng như triết lý của bóng đá Anh khi còn chơi bóng cho Leicester, dưới thời HLV Peter Taylor, năm 2001\". Bản thân Giám đốc thể thao của Lazio Oreste Cinquini cũng xác định Mancini sẽ rời đội bóng thành Rome khi mùa bóng này kết thúc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sens = test_doc.split('.')\n",
    "# for sen in sens:\n",
    "#     if len(sen) > 10:\n",
    "#         sen = gensim.utils.simple_preprocess(sen)\n",
    "#         sen = ' '.join(sen)\n",
    "#         sen = ViTokenizer.tokenize(sen)\n",
    "#         sentences.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use multiprocessing here, but we will not use it for easy in understanding code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "# sentences = []\n",
    "# train_paths = [os.path.join(dir_path, 'VNTC-master/Data/10Topics/Ver1.1/Train_Full'), \n",
    "#                os.path.join(dir_path, 'VNTC-master/Data/10Topics/Ver1.1/Test_Full'),\n",
    "#                os.path.join(dir_path, 'VNTC-master/Data/27Topics/Ver1.1/new train'),\n",
    "#                os.path.join(dir_path, 'VNTC-master/Data/27Topics/Ver1.1/new test')]\n",
    "\n",
    "# dirs = []\n",
    "# for path in train_paths:\n",
    "#     for p in os.listdir(path):\n",
    "#         dirs.append(os.path.join(path, p))\n",
    "\n",
    "# for d in tqdm(dirs):\n",
    "#     sens = get_data(d)\n",
    "#     sentences = sentences + sens\n",
    "\n",
    "# # with Pool(8) as pool:\n",
    "# #     pool.map(get_data, tqdm(dirs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(sentences, open('./sentences.pkl', 'wb'))\n",
    "sentences = pickle.load(open('./sentences.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(sentences):\n",
    "    corpus = []\n",
    "    \n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        sen = sentences[i]\n",
    "        \n",
    "        words = sen.split(' ')\n",
    "        tagged_document = gensim.models.doc2vec.TaggedDocument(words, [i])\n",
    "        \n",
    "        corpus.append(tagged_document)\n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2385532/2385532 [00:26<00:00, 88400.54it/s]\n"
     ]
    }
   ],
   "source": [
    "train_corpus = get_corpus(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_corpus = shuffle(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=40)\n",
    "model.build_vocab(train_corpus[:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_epochs = 40\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "#     print('iteration {0}'.format(epoch))\n",
    "    model.train(train_corpus[:50000],\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    \n",
    "#     # decrease the learning rate\n",
    "#     model.alpha -= 0.0002\n",
    "#     # fix the learning rate, no decay\n",
    "#     model.min_alpha = model.alpha\n",
    "\n",
    "# %time model.train(train_corpus[:50000], total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04828287,  0.25527653,  1.1613333 , -0.43151897, -0.9858117 ,\n",
       "        0.10932952,  0.20315444, -0.48530903,  0.24952224, -0.11833256,\n",
       "       -0.0337567 , -0.3887124 , -0.39426357,  0.4454976 ,  0.64964545,\n",
       "       -0.5074249 ,  0.2037328 ,  0.32153234, -0.62261915,  0.8188216 ,\n",
       "        0.5820815 , -0.09879603, -0.44826344,  0.1201525 ,  0.236654  ,\n",
       "        0.13032307, -0.46023956,  0.19788027, -0.34569028, -0.21599784,\n",
       "        0.42319658, -0.106575  , -0.24495657, -0.00839793, -0.11475623,\n",
       "       -0.5559897 , -0.12046688,  0.18673038, -0.16149993, -0.02872676,\n",
       "        0.42999822,  0.46070522,  0.50624824, -0.15866163, -0.11092521,\n",
       "        0.30938515,  0.23203233,  0.11736044, -0.7434822 , -0.78674805,\n",
       "        0.27668393,  0.25058967, -0.15513541,  0.05721006, -0.62895125,\n",
       "       -0.3618494 ,  0.48457113, -0.16074707,  0.32852057, -0.63208133,\n",
       "       -0.45503548, -0.373764  ,  0.6417061 , -0.15453526,  0.828889  ,\n",
       "        0.4040729 , -0.13313939,  0.20088702, -0.36382645,  0.3100666 ,\n",
       "        0.02355373,  0.5920582 , -0.2271741 , -0.30618507, -0.23971866,\n",
       "        0.91544545, -0.51666105, -0.05829609, -0.43708014,  0.35457942,\n",
       "        0.50872976, -0.24838248,  0.44898847,  0.11512683,  0.34157744,\n",
       "       -0.47279087, -0.02090802,  0.23195563, -0.14476988,  0.5966468 ,\n",
       "        0.25278485,  0.70205003, -0.16960798, -0.09220067,  1.387285  ,\n",
       "        0.5248568 ,  0.33318955, -0.33651793,  0.41348195, -0.94656795,\n",
       "       -0.56593996,  0.6216159 ,  0.3179036 ,  0.31106716,  0.14830516,\n",
       "        0.535672  ,  0.695546  ,  0.28968796,  0.4329898 , -0.6800865 ,\n",
       "       -0.6313374 ,  0.36142987,  0.3392832 , -0.3685879 ,  1.0465527 ,\n",
       "       -0.31610152,  0.26410806, -0.75767416, -0.0933219 , -0.10084625,\n",
       "        0.11192366, -0.63711953,  0.6878306 ,  0.20774055,  0.37814376,\n",
       "       -0.38910306, -0.29257646,  0.32447788,  1.4432929 ,  0.42116693,\n",
       "        0.10012217, -0.54671454,  0.15930349, -0.04576634,  0.11046711,\n",
       "        0.4345503 ,  0.5950319 ,  0.10390531,  0.00534402, -0.05976183,\n",
       "        1.0111569 ,  0.14526764,  0.0051693 , -0.55909073,  0.18523502,\n",
       "       -0.59934396,  0.24894848, -0.18078412,  0.5796731 , -0.44970104,\n",
       "        0.81793183, -0.5046711 , -0.16381589,  0.14662668,  0.21144816,\n",
       "        0.08799265, -0.25188333, -0.39610714, -0.46737796,  0.06498595,\n",
       "       -0.24232577,  0.08590741, -0.34991795, -0.7811069 ,  0.05049568,\n",
       "       -0.44203833, -0.04051779, -0.93674725,  0.7014623 ,  0.43860036,\n",
       "        1.0785912 ,  0.4614321 ,  0.9178922 ,  0.01267096,  0.08151802,\n",
       "       -0.21591717, -0.389159  , -0.4332839 ,  0.06478307, -0.549585  ,\n",
       "        0.24735504, -0.15430401, -0.10635387,  0.9497028 , -0.5208101 ,\n",
       "       -0.25834572,  0.5067593 , -0.3163417 , -0.45160556, -1.0110141 ,\n",
       "       -0.11357957,  0.3088588 ,  0.67771375,  0.5347725 , -0.08545431,\n",
       "       -0.6260072 ,  0.37074357,  0.3511689 ,  0.03659426, -0.5359085 ,\n",
       "       -0.22255394, -0.4841223 , -0.31908542,  0.6693267 , -0.43263623,\n",
       "        0.17883465,  0.76907945,  0.3865581 , -0.27964267,  0.5833102 ,\n",
       "        0.10791489,  0.4569784 , -0.0223736 ,  0.48295155, -0.00460218,\n",
       "       -0.47181183, -0.48191187,  0.1006198 , -0.30717742,  0.62139356,\n",
       "        0.28134045,  0.29010874, -0.26925838,  0.8383542 , -0.18886985,\n",
       "        0.18526816, -0.57650745, -0.59799755,  0.19990733,  0.22144596,\n",
       "        0.70591587, -0.76111233,  0.13711332, -0.7318054 ,  0.02516509,\n",
       "       -0.3590674 , -0.6440488 , -0.5580956 , -0.5993928 , -0.32801956,\n",
       "       -0.4644991 ,  0.89624447, -0.39741072, -0.52681875, -0.29390556,\n",
       "       -0.3324342 , -0.62701875,  0.12948091,  0.9591448 , -0.21732959,\n",
       "       -0.6216343 , -0.04387471, -0.22252487,  0.27053964,  0.17134936,\n",
       "        0.69296885,  0.39905074,  0.3307731 , -0.38610834,  0.05903669,\n",
       "        0.40507847, -0.53825825,  0.08011609, -0.27195254, -0.296355  ,\n",
       "        0.27324116,  0.5513492 ,  0.77330786, -0.6397054 , -0.24681841,\n",
       "        0.2817206 ,  0.37891504,  0.03597298,  0.42222285, -0.06389087,\n",
       "        0.39442137,  0.07020057, -0.24582939,  0.279675  ,  0.00950517,\n",
       "       -0.60586107, -1.0425315 , -0.2628614 ,  0.20990998,  0.25524455,\n",
       "       -0.27130723,  0.51966363, -0.14886895,  0.8109764 ,  0.258794  ,\n",
       "       -0.05932726, -0.10472207,  0.06371555,  0.04762143,  0.02594266,\n",
       "       -1.0294654 , -0.5873498 ,  0.60305655, -0.07507906, -0.3711069 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(train_corpus[100000].words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_sentence_vectors_from_document(doc, model):\n",
    "    vectors = []\n",
    "    sens = doc.split('.')\n",
    "    for sen in sens:\n",
    "        if len(sen) > 10:\n",
    "            sen = gensim.utils.simple_preprocess(sen)\n",
    "            sen = ' '.join(sen)\n",
    "            sen = ViTokenizer.tokenize(sen)\n",
    "            sen = sen.split(' ')\n",
    "            vec = model.infer_vector(sen)\n",
    "            \n",
    "            vectors.append(vec)\n",
    "    \n",
    "    return np.array(vectors), sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_vectors, sens = get_list_sentence_vectors_from_document(test_doc, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans = kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "avg = []\n",
    "for j in range(n_clusters):\n",
    "    idx = np.where(kmeans.labels_ == j)[0]\n",
    "    avg.append(np.mean(idx))\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, X)\n",
    "ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "summary = [sens[closest[idx]] for idx in ordering]\n",
    "\n",
    "for sen in summary:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
